{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic Modeling.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soologua/ACL2013-CharParsing/blob/master/Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU3fA0BHh04f",
        "colab_type": "text"
      },
      "source": [
        "Topic Modeling\n",
        "We use the Gensim's LDA (Latent Dirichlet Allocation) model to model topics in newsgroup_data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RngAS3HJnCJQ",
        "colab_type": "text"
      },
      "source": [
        "#Introduction#\n",
        "Suppose you have the following set of sentences:\n",
        "\n",
        "* I like to eat broccoli and bananas.\n",
        "* I ate a banana and spinach smoothie for breakfast.\n",
        "* Chinchillas and kittens are cute.\n",
        "* My sister adopted a kitten yesterday.\n",
        "* Look at this cute hamster munching on a piece of broccoli.\n",
        "\n",
        "What is latent Dirichlet allocation? It’s a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like:\n",
        "\n",
        "* **Sentences 1 and 2:** 100% Topic A\n",
        "* **Sentences 3 and 4:** 100% Topic B\n",
        "* **Sentence 5:** 60% Topic A, 40% Topic B\n",
        "* **Topic A:** 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n",
        "* **Topic B:** 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)\n",
        "\n",
        "The question, of course, is: how does LDA perform this discovery?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pCTn2NRn0Yy",
        "colab_type": "text"
      },
      "source": [
        "#LDA Model#\n",
        "\n",
        "LDA is a generative probabilistic model( a three-level hierarchical Bayesian model) for collections of discrete data such as text corpora, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document.\n",
        "In more detail, LDA represents documents as mixtures of topics that spit out words with certain probabilities. It assumes that documents are produced in the following fashion: when writing each document, you\n",
        "\n",
        "* Decide on the number of words N the document will have (say, according to a Poisson distribution).\n",
        "* Choose a topic mixture for the document (according to a Dirichlet distribution over a fixed set of K topics). For example, assuming that we have the two food and cute animal topics above, you might choose the document to consist of 1/3 food and 2/3 cute animals.\n",
        "Generate each word w_i in the document by:\n",
        "First picking a topic (according to the multinomial distribution that you sampled above; for example, you might pick the food topic with 1/3 probability and the cute animals topic with 2/3 probability).\n",
        "* Using the topic to generate the word itself (according to the topic’s multinomial distribution). For example, if we selected the food topic, we might generate the word “broccoli” with 30% probability, “bananas” with 15% probability, and so on.\n",
        "\n",
        "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDHRExsYoZuF",
        "colab_type": "text"
      },
      "source": [
        "##Example##\n",
        "Let’s make an example. According to the above process, when generating some particular document *D*, you might\n",
        "\n",
        "* Pick 5 to be the number of words in D.\n",
        "* Decide that D will be 1/2 about food and 1/2 about cute animals.\n",
        "* Pick the first word to come from the food topic, which then gives you the word “broccoli”.\n",
        "* Pick the second word to come from the cute animals topic, which gives you “panda”.\n",
        "* Pick the third word to come from the cute animals topic, giving you “adorable”.\n",
        "* Pick the fourth word to come from the food topic, giving you “cherries”.\n",
        "* Pick the fifth word to come from the food topic, giving you “eating”.\n",
        "So the document generated under the LDA model will be “broccoli panda adorable cherries eating” (note that LDA is a *bag-of-words* model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPVQJynXpVKx",
        "colab_type": "text"
      },
      "source": [
        "##Learning##\n",
        "So now suppose you have a set of documents. You’ve chosen some fixed number of **K** topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this? One way (known as collapsed **Gibbs sampling**) is the following:\n",
        "\n",
        "Go through each document, and randomly assign each word in the document to one of the K topics.\n",
        "Notice that this random assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones).\n",
        "So to improve on them, for each document d…\n",
        "Go through each word w in d…\n",
        "And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where we choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word’s topic with this probability). (Also, I’m glossing over a couple of things here, in particular the use of priors/pseudocounts in these probabilities.)\n",
        "In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
        "After repeating the previous step a large number of times, you’ll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO_N4LsJjwEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "# # from IPython.display import Image\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np17EN2C9QFc",
        "colab_type": "text"
      },
      "source": [
        "#Gensim: models.ldamodel – Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYVEoyot91Yc",
        "colab_type": "text"
      },
      "source": [
        "[This module](https://radimrehurek.com/gensim/models/ldamodel.html) allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents. The model can also be updated with new documents for online training. The core estimation code is based on the [onlineldavb.py script](https://github.com/blei-lab/onlineldavb/blob/master/onlineldavb.py), by [Hoffman, et al(2010)](https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8ol-kILmdnu",
        "colab_type": "text"
      },
      "source": [
        "#Step 1:  Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0jCrIvsTHq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "url='https://raw.githubusercontent.com/soologua/NLP-Exercises/master/2.2-topic-modeling/abcnews-date-text.csv'\n",
        "data = pd.read_csv(url,error_bad_lines=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pj8tuwsbjMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f6169bf5-2157-47e7-9d91-4e2ad2405a24"
      },
      "source": [
        "data_text = data[:300000][['headline_text']];\n",
        "data_text['index'] = data_text.index\n",
        "\n",
        "documents = data_text\n",
        "documents[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline_text</th>\n",
              "      <th>index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       headline_text  index\n",
              "0  aba decides against community broadcasting lic...      0\n",
              "1     act fire witnesses must be aware of defamation      1\n",
              "2     a g calls for infrastructure protection summit      2\n",
              "3           air nz staff in aust strike for pay rise      3\n",
              "4      air nz strike to affect australian travellers      4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF_FzRzYmsfV",
        "colab_type": "text"
      },
      "source": [
        "#Step 2: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxTA7RvEm8Q1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6c84af88-cffd-4859-d9fc-2b0f22270206"
      },
      "source": [
        "# pip install gensim\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "print np.random.seed(400)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J7EyL_unFQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dca3bf9b-cffc-4f37-eb34-65e495836cb3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBIdDD-WnXrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write a function to perform the pre processing steps on the entire dataset\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "\n",
        "# Tokenize and lemmatize\n",
        "def preprocess(text):\n",
        "    result=[]\n",
        "    for token in gensim.utils.simple_preprocess(text) :\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            # TODO: Apply lemmatize_stemming on the token, then add to the results list\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM0sTNKTnrfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "bc87211d-cb79-41f8-9289-4ba1a7ce6e94"
      },
      "source": [
        "# Preview a document after preprocessing\n",
        "\n",
        "document_num = 2310\n",
        "doc_sample = documents[documents['index'] == document_num].values[0][0]\n",
        "\n",
        "print(\"Original document: \")\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print(\"\\n\\nTokenized and lemmatized document: \")\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original document: \n",
            "['sculpture', 'exhibition', 'revealed', 'in', 'tasmania']\n",
            "\n",
            "\n",
            "Tokenized and lemmatized document: \n",
            "[u'sculptur', u'exhibit', u'reveal', u'tasmania']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Av6aGvRofKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#preprocess all the headlines, saving the list of results as 'processed_docs'\n",
        "processed_docs= documents['headline_text'].map(preprocess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qldnw-e3pqnD",
        "colab_type": "text"
      },
      "source": [
        "#Step 3: Bag of words on the dataset#\n",
        "Now let's create a dictionary from 'processed_docs' containing the number of times a word appears in the training set. To do that, let's pass processed_docs to gensim.corpora.Dictionary() and call it 'dictionary'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOM6784Hp7Ny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2hDXqllqHZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "380d5a59-9853-466b-f790-f426fbed21cc"
      },
      "source": [
        "'''\n",
        "Checking dictionary created\n",
        "'''\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7958, u'verplank')\n",
            "(13916, u'woodi')\n",
            "(14482, u'francesco')\n",
            "(17648, u'kaniva')\n",
            "(23125, u'yanagisawa')\n",
            "(12187, u'scold')\n",
            "(11412, u'szabic')\n",
            "(6913, u'emptiv')\n",
            "(23206, u'strikebreak')\n",
            "(6219, u'citrus')\n",
            "(23770, u'donger')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kxga8Ok2L36",
        "colab_type": "text"
      },
      "source": [
        "##filter_extremes##\n",
        "` filter_extremes(no_below=5, no_above=0.5, keep_n=100000)`\n",
        "* Filter out tokens that appear in less than no_below documents (absolute number) or\n",
        "more than no_above documents (fraction of total corpus size, not absolute number).\n",
        "* after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RBdBPjiqWer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "OPTIONAL STEP\n",
        "Remove very rare and very common words:\n",
        "\n",
        "- words appearing less than 15 times\n",
        "- words appearing in more than 10% of all documents\n",
        "'''\n",
        "# TODO: apply dictionary.filter_extremes() with the parameters mentioned above\n",
        "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8uFNnwk1TtZ",
        "colab_type": "text"
      },
      "source": [
        "##Doc2bow##\n",
        "Convert document (a list of words) into the bag-of-words(bow) format = list of (token_id, token_count) 2-tuples. \n",
        "Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded).\n",
        "* Create the Bag-of-words model for each document \n",
        "i.e for each document we create a dictionary reporting how many words and how many times those words appear. \n",
        "* Save this to 'bow_corpus'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl7Yin3r0pHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I25JnE5n3zK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d1c3e4ac-8f1f-4b5c-e541-c4be5bba6603"
      },
      "source": [
        "'''\n",
        "Preview BOW for our sample preprocessed document\n",
        "'''\n",
        "# Here document_num is document number 4310 which we have checked in Step 2\n",
        "bow_doc_2310 = bow_corpus[document_num]\n",
        "\n",
        "for i in range(len(bow_doc_2310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                                     dictionary[bow_doc_4310[i][0]], \n",
        "                                                     bow_doc_4310[i][1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 524 (\"reveal\") appears 1 time.\n",
            "Word 2095 (\"tasmania\") appears 1 time.\n",
            "Word 2534 (\"exhibit\") appears 1 time.\n",
            "Word 2675 (\"sculptur\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-_xm5-G4ekt",
        "colab_type": "text"
      },
      "source": [
        "#Step 4: Running LDA using Bag of Words\n",
        "We are going for **10** topics in the document corpus.\n",
        "\n",
        "Some of the parameters we will be tweaking are:\n",
        "\n",
        "\n",
        "* `num_topics` is the number of requested latent topics to be extracted from the training corpus.\n",
        "* `id2word` is a mapping from word ids (integers) to words (strings). It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
        "* `alpha` and `eta` are hyperparameters that affect sparsity of the document-topic (`theta`) and topic-word (`lambda`) distributions. We will let these be the default values for now(default value is 1/num_topics)\n",
        "\n",
        "  * `Alpha` is the per document topic distribution.\n",
        "\n",
        "    * `High alpha`: Every document has a mixture of all topics(documents appear similar to each other).\n",
        "    * `Low alpha`: Every document has a mixture of very few topics\n",
        "  * `Eta` is the per topic word distribution.\n",
        "\n",
        "    *`High eta`: Each topic has a mixture of most words(topics appear similar to each other).\n",
        "    * `Low eta`: Each topic has a mixture of few words.\n",
        "* `passes` is the number of training passes through the corpus. For example, if the training corpus has 50,000 documents, chunksize is 10,000, passes is 2, then online training is done in 10 updates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJNPjm6U4c0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = gensim.models.LdaModel(bow_corpus, \n",
        "                                   num_topics = 10, \n",
        "                                   id2word = dictionary,                                    \n",
        "                                   passes = 50)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_FusvGuCuGS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "df609916-0ed1-49d6-c3ce-e9a5fbf79e4a"
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.024*\"test\" + 0.017*\"south\" + 0.014*\"inquiri\" + 0.014*\"law\" + 0.013*\"crew\" + 0.013*\"bushfir\" + 0.013*\"hear\" + 0.013*\"studi\" + 0.012*\"research\" + 0.011*\"suspect\"\n",
            "\n",
            "\n",
            "Topic: 1 \n",
            "Words: 0.037*\"govt\" + 0.034*\"council\" + 0.034*\"plan\" + 0.033*\"urg\" + 0.030*\"water\" + 0.021*\"fund\" + 0.016*\"seek\" + 0.015*\"group\" + 0.015*\"push\" + 0.014*\"help\"\n",
            "\n",
            "\n",
            "Topic: 2 \n",
            "Words: 0.036*\"kill\" + 0.026*\"iraq\" + 0.017*\"attack\" + 0.016*\"howard\" + 0.016*\"arrest\" + 0.016*\"talk\" + 0.015*\"break\" + 0.015*\"protest\" + 0.015*\"troop\" + 0.014*\"nuclear\"\n",
            "\n",
            "\n",
            "Topic: 3 \n",
            "Words: 0.033*\"warn\" + 0.023*\"miss\" + 0.021*\"hous\" + 0.019*\"polic\" + 0.016*\"search\" + 0.015*\"rise\" + 0.015*\"industri\" + 0.015*\"claim\" + 0.014*\"centr\" + 0.013*\"water\"\n",
            "\n",
            "\n",
            "Topic: 4 \n",
            "Words: 0.024*\"chang\" + 0.021*\"labor\" + 0.020*\"opposit\" + 0.018*\"elect\" + 0.017*\"concern\" + 0.017*\"green\" + 0.016*\"price\" + 0.016*\"call\" + 0.015*\"govt\" + 0.014*\"back\"\n",
            "\n",
            "\n",
            "Topic: 5 \n",
            "Words: 0.064*\"polic\" + 0.035*\"charg\" + 0.032*\"crash\" + 0.030*\"court\" + 0.028*\"face\" + 0.023*\"investig\" + 0.019*\"accus\" + 0.018*\"murder\" + 0.017*\"drug\" + 0.014*\"woman\"\n",
            "\n",
            "\n",
            "Topic: 6 \n",
            "Words: 0.022*\"health\" + 0.021*\"minist\" + 0.019*\"want\" + 0.018*\"union\" + 0.016*\"return\" + 0.016*\"worker\" + 0.015*\"look\" + 0.015*\"work\" + 0.015*\"blaze\" + 0.013*\"welcom\"\n",
            "\n",
            "\n",
            "Topic: 7 \n",
            "Words: 0.025*\"open\" + 0.022*\"road\" + 0.022*\"lead\" + 0.021*\"australia\" + 0.020*\"jail\" + 0.020*\"death\" + 0.017*\"mayor\" + 0.016*\"coast\" + 0.016*\"take\" + 0.014*\"play\"\n",
            "\n",
            "\n",
            "Topic: 8 \n",
            "Words: 0.026*\"closer\" + 0.021*\"defend\" + 0.021*\"year\" + 0.021*\"deni\" + 0.017*\"deal\" + 0.015*\"hick\" + 0.014*\"melbourn\" + 0.013*\"releas\" + 0.012*\"govt\" + 0.012*\"announc\"\n",
            "\n",
            "\n",
            "Topic: 9 \n",
            "Words: 0.026*\"sydney\" + 0.023*\"power\" + 0.017*\"win\" + 0.014*\"lose\" + 0.014*\"final\" + 0.012*\"forc\" + 0.012*\"sale\" + 0.012*\"compani\" + 0.012*\"fall\" + 0.011*\"start\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPiNh0GE8Hj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "For each topic, we will explore the words occuring in that topic and its relative weight\n",
        "'''\n",
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjtqwsgVMzCj",
        "colab_type": "code",
        "outputId": "1553c345-ece8-4239-b879-ed40f9cf6ffc",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 344
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "datasets = files.upload()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb7f3de2-20bb-4ca6-9095-c495f03b0ad2\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cb7f3de2-20bb-4ca6-9095-c495f03b0ad2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-56381fa3e48d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/files.pyc\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/output/_js.pyc\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_message.pyc\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUL3pvyEhyxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the list of documents\n",
        "with open('newsgroups', 'rb') as f:\n",
        "    newsgroup_data = pickle.load(f)\n",
        "\n",
        "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
        "# remove tokens that don't appear in at least 20 documents,\n",
        "# remove tokens that appear in more than 20% of the documents\n",
        "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
        "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
        "\n",
        "# Fit and transform\n",
        "X = vect.fit_transform(newsgroup_data)\n",
        "\n",
        "# Convert sparse matrix to gensim corpus.\n",
        "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
        "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ0mBUsokoNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
        "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
        "ldamodel = LdaModel(common_corpus, num_topics=50, alpha='auto', eval_every=5)  # learn asymmetric alpha from data\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}